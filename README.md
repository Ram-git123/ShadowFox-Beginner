# ShadowFox-Beginner
"An end-to-end Machine Learning masterpiece using Gradient Boosting to predict Boston housing prices. Features include leak-proof Scikit-Learn pipelines, Bayesian optimization (Optuna), and Model Interpretability (SHAP)."

# ğŸ  Boston Housing: End-to-End Production ML Pipeline

This project transforms the classic Boston Housing task into a **Production-Ready Machine Learning System**. Beyond simple prediction, it implements a robust architecture focusing on **Data Integrity**, **Automated Optimization**, and **Model Interpretability**.

## ğŸ¯ Key Project Highlights

* **ğŸ† Professional Pipeline:** 100% encapsulation of preprocessing, feature engineering, and modeling using `Scikit-Learn Pipelines` to prevent **Data Leakage**.
* **ğŸ› ï¸ Advanced Feature Engineering:** Developed domain-specific features like `RM_LSTAT_ratio` (spatial-socio interaction) and `Tax_per_Room` (ownership efficiency).
* **ğŸ§ª Bayesian Optimization:** Utilized **Optuna** for hyperparameter tuning, moving beyond brute-force searches to find the optimal "Goldilocks" zone for model performance.
* **ğŸ§  Explainable AI (XAI):** Integrated **SHAP (Shapley Additive Explanations)** to deconstruct "Black Box" decisions into transparent, feature-level insights.

## ğŸ—ï¸ Architecture & Workflow

### 1. Data Audit & EDA

The initial audit identified **20 missing values** per column and a **target skewness of ~1.11**. We addressed this through **Median Imputation** and statistical scaling.

### 2. Feature Engineering Logic

We engineered features to capture non-linear relationships:

* **`RM_LSTAT_ratio`**: Captures the quality of living space relative to the neighborhood's socio-economic status.
* **`Tax_per_Room`**: Normalizes the property tax burden against house size.

### 3. Model Selection (The Shootout)

We compared four mathematical architectures using 5-fold Cross-Validation:

1. **Ridge Regression** (Linear Baseline)
2. **Random Forest** (Bagging)
3. **Gradient Boosting** (Boosting - **WINNER**)

### 4. Hyperparameter Tuning (Optuna)

Instead of a Grid Search, we used Bayesian Search to optimize:

* `learning_rate`: Log-scaled search for fine-tuned convergence.
* `n_estimators` & `max_depth`: Optimized to balance bias and variance.

## ğŸ§  Model Interpretability (SHAP)

We used SHAP values to ensure the model remains economically sound.

* **Global Importance:** Confirmed **Average Rooms (RM)** and **Lower Status Population (LSTAT)** as the primary drivers.
* **Dependence Analysis:** Identified specific inflection points where neighborhood status begins to exponentially impact price.

## ğŸ“¦ Deployment & Usage

The entire "brain" of the project is serialized in `masterpiece_boston_model.pkl`. This allows for **one-click inference** on raw data.

### How to use the Model:

import joblib

#Load the production-ready pipeline
model = joblib.load('models/masterpiece_boston_model.pkl')

#Predict instantly (preprocessing is handled automatically!)
predictions = model.predict(new_data)


## ğŸ“ Repository Structure

â”œâ”€â”€ data/               # Raw HousingData.csv & Audit results

â”œâ”€â”€ models/             # Serialized .pkl production model

â”œâ”€â”€ notebooks/          # The narrated Masterpiece.ipynb

â”œâ”€â”€ results/            # Prediction CSVs & SHAP plots

â”œâ”€â”€ requirements.txt    # Library dependencies

â””â”€â”€ README.md           # Project documentation



## âš–ï¸ License

Distributed under the **MIT License**. See `LICENSE` for more information.

-
